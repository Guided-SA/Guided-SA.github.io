<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Guided Latent Slot Diffusion for Object-Centric Learning">
  <meta name="keywords" content="Object-Centric learning, OCL, compositional learning, diffusion, generative">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Guided Latent Slot Diffusion for Object-Centric Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title section-title">Guided Latent Slot Diffusion for Object-Centric Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://kris-singh.github.io/">Krishnakant Singh</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://schaubsi.github.io/">Simone Schaub-Meyer</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.visinf.tu-darmstadt.de/visual_inference/people_vi/stefan_roth.en.jsp">Stefan Roth</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Technical Univeristy of Darmstadt,</span>
            <span class="author-block"><sup>2</sup>hessian.AI</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
          <div class="column has-text-centered">
            <span class="publication-venue">arXiv</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
     <div class="columns is-centered has-text-justified">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered" style="border-bottom: 2px solid">TL;DR &#128640</h2>
        <p>
          We present GLASS, a slot attention methods that uses language guidance in order to obtain better slot embeddings.
          GLASS sets new state-of-the-art (SOTA) benchmark for the object discovery and conditional generation tasks compared to exisiting slot attention-based methods.
        </p>
        <div class="content has-text-justified">
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="border-bottom: 2px solid">Abstract</h2>
        <div class="object-and-details">
          <img src="./static/images/teaser.png" alt="Network Diagram" loading="lazy">
        </div>
        <div class="content has-text-centered">
        <b>Left</b>: We utilize a pre-trained diffusion decoder for generating the guidance signal to obtain better slot embeddings. 
        <b>Right</b>: Our slot embeddings outperform previous slot attention methods on the task of object discovery and conditional generation, while being competitve in the property prediction task.

        </div>
 
        <div class="content has-text-justified">
          Slot attention aims to decompose an input image into a set of meaningful object
          files (slots). These latent object representations enable various downstream tasks.
          Yet, these slots often bind to object parts, not objects themselves, especially for
          real-world datasets. To address this, we introduce <b>G</b>uided <b>La</b>tent <b>S</b>lot Diffu<b>s</b>ion –
          GLASS, an object-centric model that uses generated captions as a guiding signal
          to better align slots with objects. Our key insight is to learn the slot-attention
          module in the space of generated images. This allows us to repurpose the pre-trained 
          diffusion decoder model, which reconstructs the images from the slots, as a
          semantic mask generator based on the generated captions. GLASS learns an object-level 
          representation suitable for multiple tasks simultaneously, e.g., segmentation,
          image generation, and property prediction, outperforming previous methods. For
          object discovery, GLASS achieves approx. <b>a +35% and +10% relative improvement
          for mIoU </b> over the previous SOTA method on the VOC and COCO
          datasets, respectively, and establishes a <b> new SOTA FID </b> score for conditional
          image generation amongst slot-attention-based methods. For the segmentation task,
          GLASS surpasses SOTA weakly-supervised and language-based segmentation
          models, which were specifically designed for the task.
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="border-bottom: 2px solid">Method</h2>
        <div class="object-and-details">
          <img src="./static/images/network.png" alt="Network Diagram" loading="lazy">
        </div>
        <div class="content has-text-justified">
        <b>Network architecture of GLASS</b> <span class="numberCircle" style="background-color:rgb(115,191,249);">1</span> The input image is fed to a prompt generator
        for generating a prompt P, which is obtained by concatenating generated caption and the
        extracted class label from the generated caption. <span class="numberCircle" style="background-color:rgb(241, 154, 200);">2</span>  A random noise vector, along with the generated prompt P,
        is used to generate an image using a pre-trained diffusion module. <span class="numberCircle" style="background-color:rgb(249, 219, 88);">3</span> The cross-attention
        layers of the diffusion model, along with self-attention layers, are used in the pseudo ground-truth
        generation module to generate the semantic mask. <span class="numberCircle" style="background-color:rgb(152, 249, 234);">4</span> The generated image is passed
        through an encoder model (DINOv2) followed by a slot attention module to generate slots. <span class="numberCircle" style="background-color:rgb(237, 110, 88);">5</span> The
        slots are matched with their corresponding object masks using the Hungarian matcher
        module.<span class="numberCircle" style="background-color:rgb(164, 247, 105);">6</span> The slot attention module is trained end-to-end using the mean squared error between the
        reconstructed and the generated image and our guidance loss between the predicted
        mask from the slots and their matched object mask. GLASS is trained on generated
        images only, the real images are only used for prompt generation.

        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="border-bottom: 2px solid">Results</h2>
        <div class="content has-text-justified">
        <p> GLASS and GLASS<sup>&dagger;</sup> are representaiton learning methods that perform multiple downstream tasks like object discovery (OD), property prediction (pp), and conditional generation (CG). 
          We show results for all the tasks.
        </p>

      </div>
    </div>
   </div>
    <!--/ Matting. -->

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Object Discovery</h2>
        <div class="content has-text-justified">
          <p>
            We compare our method against language-based segmentation models (top partition), weakly-supervised models (middle
            partition), and OCL models (bottom partition) for the object discovery task. Downstream Tasks
            denote a model’s capability for solving the following tasks: <b>OD:</b> object discovery, <b>PP:</b> object-level
            property prediction, and <b>CG:</b> conditional generation. Input denotes the input signal the model trains
            on, where <b>I:</b> image, <b>C:</b> captions, and <b>L:</b> image-level labels. The best value is highlighted in red,
            the second best in blue. We show the relative improvement (in parentheses) of GLASS compared to
            the best OCL method. Our method outperforms all models on the COCO dataset and is comparable
            to the best model on the VOC dataset (ToCo).
          </p>
          <div class="is-centered is-full-width">
          <img src="./static/images/od_quant.png">
          </div>
         </div>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Results</h2>

        <div id="wrapper">
          <img src="./static/images/od_qualt.png" width="450px">

        </div>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">OD with other weakly-supervised OCL methods</h2>
        <div class="content has-text-justified">
          <p>
            GLASS outperforms other weakly-supervised variants of StableLSD such as <b>StableLSD-BBox:</b> Use bounding box for initalizing slots and <b>StableLSD Dynamic:</b> Use a dynamic number of slots equal to number of objects present in the scene. 
          </p>
        <div id="wrapper">
          <img src="./static/images/od_quant_wsl.png" width="450px">
        </div>
        </div>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conditional Generation</h2>
        <div class="content has-text-justified">
          <p>
            GLASS and GLASS<sup>&dagger;</sup> outperform StableLSD on the task of conditional generation in terms of FID on the COCO and VOC datasets.
          </p>
          <div id="wrapper">
          <img src="./static/images/cond_gen_quant.png" width="350px">
          <img src="./static/images/cond_gen_qual.png" width="400px">
          </div>
        </div>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Object-level propery prediction</h2>

        <div class="content has-text-justified">
          <p>
            An ideal model should have a high detection rate and accuracy. GLASS and GLASS<sup>&dagger;</sup> have a slightly lower
            (approx. -2%) accuracy drop but have a higher increase in detection rate compared to StableLSD.
            Here, □ and ♢ show the results for the COCO and VOC datasets, respectively
          </p>
          <div id="wrapper">
            <img src="./static/images/downstream.png" width="450px">
          </div>
        </div>
      </div>
    </div>
</div>


</div>

</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{singh2024synthetic,
  author    = {Krishnakant Singh and Simone Scahub-Meyer and Stefan Roth},
  title     = {Guided Latent Slot Diffusion for Object-Centric Learning},
  booktitle = {arxiv cs.CV},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
      The website template is borrowed from the awesome <a href="https://github.com/nerfies/nerfies.github.io"> Nerfies</a> website.
      </p>
        </div>
      </div>
    </div>
   </div>
  </div>
</footer>

</body>
</html>
